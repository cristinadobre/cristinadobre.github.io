# selection of projects, talks, awards
<br><br>


## LBW at CHI2022
*April 2022*

![TeaserPicturefromCHIPaper](/assets/img/CHI22.png) 

An initial research outcome from the summer 2021 research project with [MSR Cambridge](https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/) is going to be published at [CHI2022](https://programs.sigchi.org/chi/2022/program/content/73059)! The article is entitled: [Nice is Different than Good: Longitudinal Communicative Effects of Realistic and Cartoon Avatars in Real Mixed Reality Work](https://www.microsoft.com/en-us/research/uploads/prod/2022/03/Dobre-et-al-2022-AvatarAppearance-CHILBW-CameraReady.pdf), and more information will be soon available.

---

## Journal article at Springer VR
*March 2022*

![TeaserMLPipelinefromPaper](/assets/img/SpringerVR22.png) 

The research outcome that started with my first internship in 2019 is officially out! We investigated an immersive ML for social attitude detection in VR narrative games.

The work is published with [Springer Virtual Reality](https://www.springer.com/journal/10055), and the full paper can be found here: [Immersive machine learning for social attitude detection in virtual reality narrative games](https://link.springer.com/article/10.1007/s10055-022-00644-4)

Special thanks to my supervisors and co-authors [Sylvia Pan](https://www.panxueni.com/) and [Marco Gillies](https://www.gold.ac.uk/computing/staff/m-gillies/), to the industry collaborators: [Dream Reality Interactive](https://www.dreamrealityinteractive.com/) and [Maze Theory](https://www.maze-theory.com/). The teaser video is upcoming!

---

## ICMI 2021
*October 2021*

I presented my work at the [International Conference on Multimodal Interaction](https://icmi.acm.org/2021/) (ICMI) on the gaze dynamics in dyads during an unstructured conversation using multimodal data. This work was in collaboration with [Antonia Hamilton’s Lab](http://www.antoniahamilton.com/) who kindly provided me with the data.

The main results were: (1) Listeners look more at the other person than speakers.(2): When looked at, people switch between direct gaze and avert gaze with a higher frequency compared to when they are not looked at.(3): When not looked at people look more at the other person’s face (direct gaze) than somewhere else (avert gaze).

You can read the full paper here: [Direct Gaze Triggers Higher Frequency of Gaze Change: An Automatic Analysis of Dyads in Unstructured Conversation](https://research.gold.ac.uk/id/eprint/30683/1/3462244.3479962.pdf)

![ThePosterfromICMI](/assets/img/ICMI21_Poster.jpg) 

---

## Internship at Microsoft Research Cambridge
*Summer 2021*


Over the summer of 2021, I had the opportunity to meet and work alongside a super talented and ambitious group of people from [Microsoft Research Cambridge](https://www.microsoft.com/en-us/research/lab/microsoft-research-cambridge/). For four months we cooked up a cutting-edge research project that would impact and feed into the next developments of collaboration in immersive environments (VR/MR).

I was part of the [Future of Work group](https://www.microsoft.com/en-us/research/theme/future-of-work/), supervised by [Sean Rintel](https://www.microsoft.com/en-us/research/people/serintel/). I was very closely collaborating with [Marta Wilczkowiak](https://www.linkedin.com/in/marta-wilczkowiak-716b772) from the HoloLens group. The aim of the project was to understand how the looks of avatars impact the interaction in virtual environments. The current work in this area is not conclusive and there’s no literature from longitudinal studies.

Thus, we rolled up our sleeves and designed a study comparing two types of personalised, full-body avatars. We considered cartoon-like and realistic avatars, all participants having their own avatars (in two styles!). Participants would run their usual, recurrent meetings every day for two weeks swapping the avatars halfway through. All meetings were in Mixed Reality (MR), using the [HoloLens 2](https://www.microsoft.com/en-gb/hololens/hardware) device.

After this design period, the more hands-on part started. It involved implementing the MR environment for the networked meeting, creating a workflow for animating full-body avatars, building a telemetry system, recruiting participants from different departments and time zones. I screened 30+ participants out of which we recruited 14 (7 female, 6 male, 1 non-binary) forming 6 groups. Each participant was using a HoloLens 2 device and could see and interact in real-time with their colleagues' full-body avatar.

This project was such an enriching experience! My PhD research was mostly focused on VR, hence I got first-hand experience with MR applications/devices and the differences from the VR ones. I would love to work on a project combining both VR and MR mediums!

This was also my first longitudinal study. I have got an even clearer understanding of the benefits of prolonged exposure in the area of immersive virtual environments (MR/VR). Going forward, I am keen on designing more longitudinal research studies and I’ll appreciate much more the published longitudinal research out there!

![SelfieWearingHL2](/assets/img/WearingHL2.jpg) 

---

## Workshop at IVA2020
*October 2020*


![IVAConference](/assets/img/Oct20IVAinVR.png) 

The [20th International Conference on Intelligent Virtual Agents](https://iva2020.psy.gla.ac.uk/) (IVA) was supposed to take place in Glasgow, but like all other conferences this year, it happened online. We presented there a workshop on [Immersive Interaction design for Intelligent Virtual Agents](https://iva2020.psy.gla.ac.uk/program/workshops/workshops/). It looked at how approaches to interaction design for body movement can be applied to interaction with IVAs, in particular to non-verbal communication with virtual agents. It was a hands-on workshop where [Nicola Plant](http://nicolaplant.co.uk/) and Clarice Hilton introduced a new interactive machine learning tool, [InteractML](http://interactml.com/). Those who attended used their own body movements to explore new ways of interacting with virtual agents and discovered InteractML, that allowed for rapid implementation of these designs.

The organisers of IVA chose [Gather.Town](https://gather.town/) platform to host the event. From my perspective, it went really well. I enjoyed being able to *move* so that I can talk to people and to make sure to be in the auditorium on time for the main talk sessions. They also had [Salons](https://iva2020.psy.gla.ac.uk/program/salons/) where you'd stay at the same table with others and have a discussion driven by a senior member of the community. I enjoyed these the most and I think they brought a bit of that networking feeling from being at a in-person conference.

Here I also got to meet [Hannes Högni Vilhjálmsson](http://www.ru.is/faculty/hannes/) whose research is very interesting and diverse; Along with other organisers, he worked hard to make sure the conference went smoothly and also organised a VR afterparty with a Halloween theme. 


---

## Internship at Microsoft Research with Jaron Lanier
*June 2020*


I was very excited to have the chance to work as an intern with [Jaron Lanier](http://www.jaronlanier.com/) at the Office of the CTO at [Microsoft Research](https://www.microsoft.com/en-us/research/). Along with [M Eifler](http://www.blinkpopshift.com/) and others in the lab, we were supposed to work on a very interesting project that I probably can't say much about. Due to the COVID-19 pandemic, my internship had to be postponed to summer 2021. Unfortunately, later in the year, Microsoft Research decided to cancel the offered internships of those not residing in the US or Canada. For as much I would have enjoyed and learned during this internship, it couldn't happen at these challenging times. Hopefully, there will be a way to cross paths and work together with Jaron Lanier at some point in the future.

---

## Award: Google Women Techmakers Scholarship
*April 2020*

![GoogleWomenTechmakersScolarship](/assets/img/April20GoogleScholarship.PNG)

I was so happy to find out that I was awarded the [Google Women Techmakers](https://www.womentechmakers.com/) Scholarship. Unfortunately, the trip to the US and the workshop were cancelled due to COVID-19 pandemic. There we would have met [all other scholars](https://buildyourfuture.withgoogle.com/scholarships/google-scholarship-recipients/) and we would have had workshops and networking events. We had instead a very nicely organised online workshop where we met most of the scholars and we've got a very interesting talk from [Jade Raymond](https://en.wikipedia.org/wiki/Jade_Raymond), VP and Head of Stadia Games and Entertainment. 

---

## Webinar at Immersive UK
*April 2020*

![ImmersiveUKTalk](/assets/img/April20ImmerseUKTalk.gif)

Together with [Sylvia Pan](https://www.panxueni.com/) and [Marco Gillies](https://www.gold.ac.uk/computing/staff/m-gillies/), we presented some of our work on AI-driven characters for virtual environments. The [webinar](https://www.immerseuk.org/news/registration-now-open-for-our-ai-driven-characters-lunchtime-live-webinar/) scratched the surface on how we can develop and drive virtual characters in interactions in VR, based on what we know from an academic and industry perspective.  

---

## Award: Rabin Ezra Scholarship 
*March 2020*

<p align="center">
  <img src="/assets/img/March20RabinEzra.png" />
</p>


I was awarded the [Rabin Ezra Scholarship](http://rabinezrascholarship.org/) which supports junior researchers. The Trust awards scholarships each year and they usually have the deadline at the beginning of December. 

---

## Winner of VIVE Dev Jam
*January 2020*

![Jan20 ViveJam](/assets/img/Jan20ViveJam.jpg)

Our Pinky Mega team 4 (very spontaneous name, I know) won the [first prize](https://twitter.com/htcvive/status/1221865150916882433) at the VIVE developer Jam. 

The team was formed of [Carlos Gonzalez Diaz](https://carlotes247.github.io/), Claire Wu, [Lili Eva Bartha](https://www.lilievabartha.com/) and myself and we put together an active listening project where the user would drive the interaction (and the discourse) using their implicit behaviour. 

It was a lot of fun but also long hours of work. We were very sleep deprived after the whole weekend but happy that we got the chance to meet new people and to play with some pretty cool new toys such as the lip tracker from VIVE. Goldsmiths University of London wrote [a blog post](http://www.doc.gold.ac.uk/blog/?p=2886) about us and other Goldsmiths students who attended the Jam.

---

## Speaker at the *AI and Character Driven Immersive Experiences Workshop*
*December 2019*

![Dec19 Workshop](/assets/img/dec19workshopcombined.PNG)

I ended 2019 with a [workshop](https://sites.google.com/view/gsvr) on how immersive experiences can be driven by AI and virtual characters. As soon as I finished my internship, I started helping plan this workshop with [Sylvia Pan](https://www.panxueni.com/) and [Marco Gillies](https://www.gold.ac.uk/computing/staff/m-gillies/). It was an even that showed work from the academia (such as the work of [Prof Anthony Steed](https://wp.cs.ucl.ac.uk/anthonysteed/) or [Prof Antonia Hamilton](http://www.antoniahamilton.com/)) but also from the industry world (Royal Shakespeare Company, or HTC VIVE). 

I also had the chance to give a full presentation (and a demo) on the work I've done during my internship. It included the things I've learned and the AI tool we've developed. More on this to come, hopefully as a more polished project and with the published results. 

---
<br>


## Poster and DC at ACII19
*September 2019*

![ACII Talk](/assets/img/acii_talk.jpg)

I've been following the research presented at [ACII](http://acii-conf.org/2019/) (International Conference on
Affective Computing & Intelligent Interaction) since the start of my research journey. I'm so happy to have the chance to present some of my preliminary work as a [doctoral consortium paper](https://ieeexplore.ieee.org/document/8925113) and poster. 

I have to say, I was very nervous about presenting it, but after that, I had lots of fun attending the rest of the conference. I could see in person some of the researchers I follow a lot such as [Catherine Pelachaud](http://pages.isir.upmc.fr/~pelachaud/), [Stacy Marsella](http://stacymarsella.org/Stacy_Marsella_Homepage/Stacy_Marsella_Homepage.html), [Jonathan Gratch](https://people.ict.usc.edu/~gratch/), [Rosalind Picard](https://www.media.mit.edu/people/picard/publications/), [Daniel McDuff](http://alumni.media.mit.edu/~djmcduff/) and many others. 

There were three keynote speakers Simon Baron-Cohen, Lisa Feldman Barrett and Thomas Ploetz. All three were very insightful, however, Lisa Feldman Barrett gave a very on-point presentation on the machines perceiving emotions. [The keynote recording](https://www.youtube.com/watch?v=gPPHgeJHRF8) is on YouTube; do have a look if you're interested in this subject. After the conference, I read her book [*How Emotions are Made*](https://lisafeldmanbarrett.com/books/how-emotions-are-made/) which was a very very good and easy-to-follow book. 


---
<br>

## Imitation Learning Workshop 
*September 2019*

![IGGI Workshop talk](/assets/img/iggi19workshop.jpg)

Along with [Carlos Gonzalez Diaz](https://carlotes247.github.io/), we organised a workshop for the annual *Intelligent Games & Game Intelligence ([IGGI](https://iggi.org.uk/)) Conference* in York, UK. We introduced the Unity3D ML-Agents platform, the algorithms that can be used within the framework with a focus on the imitation learning one: Behaviour Cloning. The attendees went through a tutorial to build a game by setting-up the ML-Agents framework, training it using Tensorflow and testing it back in the game engine. 

If you're interested, you can have a look at the [slides presented](https://docs.google.com/presentation/d/1Q7S3RcUPYF3c8m5Y22ruEv24_OBPnffpIoDStV60VY4/edit?usp=sharing) and at the [github repo](https://github.com/carlotes247/IGGI19_Imitation_Learning_Workshop) used in the workshop.

---
<br>


## Doctoral Research Intern
*Summer-Autumn 2019*

![Internship at DRI](/assets/img/DRI_working_PC.jpg)

Collaborating with two games studios ([Dream Reality Interactive](https://www.dreamrealityinteractive.com/) and [Maze Theory](https://www.maze-theory.com/)) and with the academic support from Goldsmiths UoL, we created an immersive ML pipeline for Virtual Reality experiences, project funded by [Innovate UK](https://www.ukri.org/councils/innovate-uk/). 

The work was based on reinforcement learning and imitation learning concepts using the [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents) platform. The ML pipeline we built detects a social attitude (such as sympathy, aggression or social engagement) during an interaction in VR between the player and a non-player character. Together we conceptualised the pipeline, planned the study design, built the environment for data collection, trained & evaluated the model and part-integrated the pipeline in the final game product. 


<p align="center">
  <img src="/assets/img/DRI_working_VR.gif" />
</p>
<!-- ![Internship at DRI_gif](/assets/img/DRI_working_VR.gif) -->

Overall, this work introduces an immersive ML pipeline for detecting social attitudes and demonstrates how creatives could use ML and VR to expand their ability to design more engaging experiences. Here, Goldsmiths wrote [a news article](https://www.gold.ac.uk/news/peaky-blinders-vr/) about it.


---
<br>

## Teaching 3D Virtual Environments and Animation
*Spring Term 2019*

This spring term I was an associate lecturer for the VR module in Goldsmiths. Along with [Leshao Zhang](https://twitter.com/zhangleshao?lang=en), I took care of the labs, while [Sylvia Pan](https://twitter.com/panxueni) was running the lectures. I was teaching mostly basics of Unity3D for VR applications and helping students with VR projects (just imagine lots of brainstorming, troubleshooting, code debugging and testing cool(!!) VR games and applications). They came up with really cool projects, there will be a blogpost about this soon! For now have a short list of some of them: Zombie Game, Furniture assembly, Bee Simulator(short video below), Stress-relieve smashing game, Water Meditation and Immersive 3D creation and visualisation tool. Marco Gillies wrote a more in-depth [blog post on Medium](https://medium.com/virtual-reality-virtual-people/vr-masters-projects-de45175e180c) about it.

<!-- ![bee-simulator](/assets/img/bee-simulator.gif) -->
<!--  <p><center><video src="/assets/img/bee-simulator.mp4" width="260" height="480" controls preload></video></center></p> -->

<p align="center">
  <img src="/assets/img/bee-simulator.gif" />
</p>


I hope the students benefited a lot from this module, it was very interesting for me too. It was also my first teaching experience in a university environment- nothing like teaching kids creative coding in afterschool clubs!! A few weeks ago it had a guest speaker too -[Francesco Giordana](https://twitter.com/fragiordana?lang=en) from [Moving Picture Company](https://www.mpcfilm.com/). It was incredible to see that they are using VR for film making; He described something called *Virtual Production* which lets previewing Computer-Generated (CG) elements, in real-time on a film set. It also enables the filmmakers to scout the environment, capture, add animations iterate through and shoot with virtual cameras - everything **within** the virtual environment! They had projects where all of this was done remotely too- with people from around the world.

The 3D Virtual Environments and Animation module will run again next year- I look forward to being part of this again. There's also a brand new [MA/MSc Virtual and Augmented Reality](https://www.gold.ac.uk/pg/ma-msc-virtual-augmented-reality/) that starts next September as well, if it sounds like your thing! 

---
<br>

## Virtual Social Interaction 
*December 2018*

The [4th Workshop on Virtual Social Interaction](https://sites.google.com/view/vsi2018/home) took place on 17-18 Decembre 2018 at Goldsmiths UoL. I'm glad I had the chance to attend and help organise this event. 

[Marco Gilles](http://www.doc.gold.ac.uk/~mas02mg/MarcoGillies/) wrote a nice overview of the event- have a look at his [VSI18 blogpost](https://medium.com/virtual-reality-virtual-people/virtual-social-interaction-conference-8972dcd9211c). It was indeed, such an interesting workshop across both days, featuring all female keynote speakers, a range of inspiring talks and a buzzing poster session.

If you joined the workshop and want to stay in touch (or if you want to hear updates about it + other related news) join our [VSI Google Group](https://groups.google.com/forum/embed/?place=forum/virtual-interaction)! 

Oh, and stay tuned for **VSI2020**!!

---
<br>


## W|E
*August 2018*

![W\|E project: Trabi](/assets/img/wetrabi.png)

In August 2018 I took part in this amazing one-week [Signal & Noise workshop](https://www.media.mit.edu/events/mlberlin-signalandnoise/) organised by the [MIT Media Lab](https://www.media.mit.edu/) in Berlin. The track I was part of was  *VR/AR-based learning experience* led by [Scott Greenwald](https://www.media.mit.edu/people/swgreen/projects/). 

We created, indeed, a learning experience about the Berlin Wall based on first-hand escape stories. Media Lab created this [video to showcase the behind-the-scenes process](https://youtu.be/C16gaWamXpg) of our project and Die Welt wrote [this article](https://www.welt.de/kmpkt/article181321644/MIT-Projekt-Wie-du-unter-der-Berliner-Mauer-in-den-Westen-fluechten-kannst.html) about our work! One week was enough to prototype this, but we want to put together a final product, therefore this is an on-going project now- _how exciting_! Find out more about the project and the (great!) people working on it on [W\|E website](http://we-vr.berlin/) and follow us [on Twitter](https://twitter.com/WEVR_Berlin) to see what we are up to! 

---
<br>

## VR Escape Room experience
*May 2018*

![VR Escape Room](/assets/img/vrthingy.PNG)

This is a game prototype for an escaperoom/puzzle-like experience in 6DoF VR. The aim is to exit the room by opening the main door. The main door is initially locked (shown by a red door). To unlock it, the user needs to solve a puzzle. It can be solved by finding the right objects in the room and place them on the table next to their 'trace'. The shapes on the table and the clue on the screen should help the player pick the right objects and solve the puzzle. When the right objects are placed on the table, the door gets unlocked (its colour turns green) and the player can open it and exit the room. Here's a [short overview of how the room looks like](https://www.youtube.com/watch?v=cxV6fWVhEZE).

This prototype was created by myself in Unity 3D using Oculus SDK and VRTK for the Game Development module (14/05/18 - 25/05/18).

---
<br>

## GGJ Game: QT-Labs
*Jan 2018* 

![QT-Labs](/assets/img/qtlabs.PNG)

This is a VR game created in Unity3D over a weekend during the 2018 Global Game Jam (Queen Mary UoL site). The team had six members: one sound effect & music member, two 3D and 2D artists who created and designed the game assets and three developers. I was one of the three developers, having the role of implementing the game logic and testing the game. Here's our [porject overview on GGJ website](https://globalgamejam.org/2018/games/qt-labs) and here's a [short gameplay.](https://vimeo.com/253292962)

---
<br>

## StarWing Genetica
*Nov 2017* 

![StarwingGenetica](/assets/img/starwinggenetica.PNG)

This is a game developed in Unity over the course of a 2-weeks Game Development module in Goldsmiths UoL. StarWing Genetica is a one player space-themed shooter game, partly inspired by classic arcade games. In this game, a genetic algorithm is used to evolve better enemy agents over time. [Here's a short gameplay.](https://www.youtube.com/watch?v=n4fnTExbxMs&feature=youtu.be)

It was designed and built by a team of three: Charlie Ringer, Valerio Bonametti, and myself.

---
<br>

*So what now? Maybe check out [what I'm doing now](https://cristinadobre.github.io/now.html) or see some basic stuff [about me](https://cristinadobre.github.io/)!*
